{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашняя работа по регуляризации и оптимизации\n",
        "\n",
        "Ниже приводится корпус данных с двумя метками: 1 и -1. К данным применяется линейная модель классификации:\n",
        "\n",
        "$f(x, \\theta) = x_1 \\theta_1 + x_2 \\theta_2 + \\theta_3.$\n",
        "\n",
        "Предлагается подобрать параметры $\\theta$ минимизируя следующую функцию ошибки:\n",
        "\n",
        "$\\mathcal{L}(\\theta) = 0.1 \\|\\theta\\|^2 + \\frac{1}{N}\\sum\\limits_{i=1}^N \\max(0, 1 - y_i f(x_i, \\theta)).$\n",
        "\n",
        "Для оптимизации предлагается использовать метод градиентного спуска с 1000 шагами размера $0.1$ из начальной точки $(1, 1, 0)$."
      ],
      "metadata": {
        "id": "l9Ah7xJfbQBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Я как-то увлекся и начал писать на английском. \n",
        "\n",
        "Сбивающие с толку обозначения переменных $x_i$, так как в функции $f$ индексация обозначает элемент столбца $\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, а в функции потерь $x_i$ обозначает номер пакета данных, т.е. номер самого столбца из двух иксов. В качестве обозначения столбца будем использовать $X_j$, а вместо $y_i$ использовать $Y_j$."
      ],
      "metadata": {
        "id": "qeUaHZZQ5_dZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HruK4RFynXYJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = np.array([0, 1, 1, -0.5, 0], dtype='float64')\n",
        "X2 = np.array([1, 1, 0, 0.5, -0.5], dtype='float64')\n",
        "X3 = np.ones(5, dtype='float64') # for convience\n",
        "X = np.array(list(zip(X1, X2, X3)))\n",
        "Y = np.array([1, 1, 1, -1, -1], dtype='float64')"
      ],
      "metadata": {
        "id": "y8O-pwl7qowf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам будет удобнее считать в коде, что существует $x_3$ для каждого сампла, всегда равный 1. \\\\\n",
        "Оно не будет менять результат."
      ],
      "metadata": {
        "id": "aZdaDZUR9Duy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, theta):\n",
        "  return x[0]*theta[0] + x[1]*theta[1] + x[2]*theta[2]"
      ],
      "metadata": {
        "id": "Cew5DmYMpEog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(theta, X, Y):\n",
        "  N = len(X)\n",
        "  max_arr = np.array([max(0, 1 - Y[i] * f(X[i], theta)) for i in range(N)])\n",
        "  mean = (1/N) * np.sum(max_arr)\n",
        "  return 0.1 * np.linalg.norm(theta)**2 + mean"
      ],
      "metadata": {
        "id": "iEsJmygRnvXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_start = np.array([1, 1, 0], dtype='float64')\n",
        "loss(theta_start, X, Y)"
      ],
      "metadata": {
        "id": "xCi-qc_MrF_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Считаем градиент лосса\n",
        "$$\\mathfrak{L}(\\theta) = 0.1 ||\\theta||^2 + \\frac{1}{N} \\sum^N_{i=1} \\max(0, 1-Y_i f(X_i, \\theta))  $$\n",
        "\n",
        "$$f(x, \\theta) = x_1 \\theta_1 + x_2 \\theta_2 + \\theta_3 $$\n",
        "\n",
        "We should compute gradient in respect to θ. \n",
        "\n",
        "Let $g(z(\\theta)) = max(0, 1 - y z(\\theta)), \\ \\ z(\\theta) = f(X, \\theta)$ \\\\\n",
        "\n",
        "First, compute $z(\\theta)$ gradient: \\\\\n",
        "$$\\frac{∂z}{∂θ_1} = x_1$$\n",
        "$$\\frac{∂z}{∂θ_2} = x_2$$\n",
        "$$\\frac{∂z}{∂θ_3} = 1$$\n",
        "$$\\nabla z = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ 1 \\end{pmatrix}$$\n",
        "\n",
        "Using chain rule:\n",
        "$$\\frac{∂}{∂θ}g(z(\\theta)) = \\frac{∂g}{∂z}\\frac{∂z}{∂θ}$$\n",
        "\n",
        "Then \\\\\n",
        "$$\\xi^i := \\frac{\\partial g(z(\\theta))}{∂\\theta_i} = \n",
        "\\begin{cases}\n",
        "  0, & y z > 1 \\\\\n",
        "  - y x_i, & y z < 1\n",
        "\\end{cases}$$\n",
        "\n",
        "(assuming $x_3$ = 1) \\\\\n",
        "$-y$ comes from g's deriative, $x_i$ is from inner z's deriative.\n",
        "\n",
        "Then compute mean over all data packs.\n",
        "\n",
        "$$\\frac{∂}{∂θ_i}\\frac{1}{N} \\sum^n_{j=1} \\max(0, 1-Y_j f(X_j, \\theta_i)) = \\frac{1}{N} \\sum_{j=1}^N \\xi^i_j = \\frac{1}{N} \\sum_{j=1}^N \\begin{cases} 0 & Y_j f(X_j, \\theta) > 1 \\\\ - Y_j X_{ji}, & Y_j f(X_j, \\theta) < 1\\end{cases}$$\n",
        "\n",
        "There $X_{ji}$ means $x_i$ from $X_j$ data column.\n",
        "\n",
        "Let's find $||\\theta||^2$ norm's deriative:\n",
        "$$||\\theta||^2 = \\theta_1^2 + \\theta_2^2 + \\theta_3^2 $$\n",
        "\n",
        "$$\\frac{\\partial ||\\theta||^2}{∂θ_i} = 2\\theta_i$$\n",
        "$$\\nabla ||\\theta||^2 = \\begin{pmatrix} 2θ_1 \\\\ 2θ_2 \\\\ 2θ_3 \\end{pmatrix}$$\n",
        "\n",
        "And finally\n",
        "$$\\frac{\\partial \\mathfrak{L}}{∂\\theta_i} = 0.2 \\theta_i + \\frac{1}{N} \\sum_{j=1}^N \\xi^i_j$$"
      ],
      "metadata": {
        "id": "j_A0of5OwX2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = np.array([1, 1, 0], dtype='float64')"
      ],
      "metadata": {
        "id": "wC-MuVstv8J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(theta, X, Y, lr):\n",
        "  grad = np.zeros(3)\n",
        "  for i in range(len(theta)):\n",
        "    grad[i] = 0.2 * theta[i]\n",
        "\n",
        "    N = len(X)\n",
        "    for j in range(N):\n",
        "      der_mean = 0\n",
        "      if (Y[j] * f(X[j], theta) < 1):\n",
        "        der_mean += Y[j] * X[j][i]\n",
        "      grad[i] -= (1/N) * der_mean\n",
        "\n",
        "  return lr * grad"
      ],
      "metadata": {
        "id": "a53fZoopuEHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_descent(theta, X, Y, steps=10000, lr=0.1):\n",
        "  for _ in range(steps):\n",
        "    theta -= step(theta, X, Y, lr)"
      ],
      "metadata": {
        "id": "6IMh0wBq-BgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_descent(theta, X, Y)"
      ],
      "metadata": {
        "id": "TD6m_toM7f0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func(X, theta):\n",
        "    theta = np.asarray(theta)\n",
        "    return (X * theta[:2]).sum(axis=-1) + theta[2]\n",
        "\n",
        "print(\"Prediction:\", func(list(zip(X1, X2)), theta))\n",
        "print(\"Loss:\", loss(theta, X, Y))\n",
        "\n",
        "with open(\"submission.yaml\", \"w\") as fp:\n",
        "    yaml.safe_dump({\"tasks\": [{\"task1\": {\"answer\": theta.tolist()}}]}, fp)"
      ],
      "metadata": {
        "id": "OiWFbPJabowP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}